{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from stable_baselines3 import PPO, SAC, TD3, DDPG, A2C, DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from gym_donkeycar.envs.donkey_env import DonkeyEnv\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, CallbackList\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecTransposeImage\n",
    "\n",
    "from sb3_contrib import TQC\n",
    "\n",
    "import sys\n",
    "\n",
    "from torch.utils.data.dataset import Dataset, random_split\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import gym\n",
    "\n",
    "path = r\"D:\\Memoire-loic-RL\\gym-donkeycar\\CarConsumptionModel\"\n",
    "sys.path.insert(0, path)\n",
    "\n",
    "\n",
    "from donkey_environment.ConsumptionWrapper import ConsumptionWrapper\n",
    "from utils.callbacks import CustomProgressBarCallback\n",
    "from utils.ExpertDataset import ExpertDataSet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.8\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#! pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# print(th.version.cuda)\n",
    "# print(th.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creation of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting DonkeyGym env\n",
      "Setting default: start_delay 0.0\n",
      "Setting default: max_cte 8.0\n",
      "Setting default: frame_skip 1\n",
      "Setting default: cam_resolution (120, 160, 3)\n",
      "Setting default: log_level 20\n",
      "Setting default: host localhost\n",
      "Setting default: port 9091\n",
      "Setting default: steer_limit 1.0\n",
      "{'throttle_min': 0.0, 'throttle_max': 1.0, 'level': 'steep-ascent', 'start_delay': 0.0, 'max_cte': 8.0, 'frame_skip': 1, 'cam_resolution': (120, 160, 3), 'log_level': 20, 'host': 'localhost', 'port': 9091, 'steer_limit': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\openhub\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\spaces\\box.py:78: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "c:\\Users\\openhub\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#env = ConsumptionWrapper(level=\"steep-ascent\", conf={\"throttle_min\" : 0.0, \"throttle_max\" : 1.0})\n",
    "\n",
    "env_kwargs = {\n",
    "    \"level\": \"steep-ascent\",\n",
    "}\n",
    "\n",
    "conf = {\n",
    "    \"throttle_min\" : 0.0,\n",
    "    \"throttle_max\" : 1.0\n",
    "}\n",
    "\n",
    "env_kwargs.update({\"conf\": conf})\n",
    "\n",
    "env = make_vec_env(\n",
    "    ConsumptionWrapper, \n",
    "    n_envs=1, \n",
    "    env_kwargs=env_kwargs, \n",
    "    seed=42,\n",
    "    vec_env_cls=DummyVecEnv,\n",
    ")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalization methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Action: tensor([0.5000, 0.5000], dtype=torch.float64)\n",
      "Denormalized Action: tensor([0.5000, 0.5000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def normalize_actions(action: np.ndarray) -> np.ndarray:\n",
    "     # [-1, -1], [1,1]\n",
    "    \n",
    "    low, high = np.array([-1, -1]), np.array([1, 1])\n",
    "    predicted_action = 2 * (action - low) / (high - low) - 1\n",
    "    return predicted_action\n",
    "\n",
    "def denormalize_actions(normalized_action: np.ndarray) -> np.ndarray:\n",
    "    low, high = np.array([-1, -1]), np.array([1, 1])\n",
    "    denormalized_action = (normalized_action + 1) / 2 * (high - low) + low\n",
    "    return denormalized_action\n",
    "\n",
    "\n",
    "# Example action\n",
    "action = np.array([0.5, 0.5])\n",
    "# convert to tensor\n",
    "action = th.tensor(action)\n",
    "\n",
    "# Normalize the action\n",
    "normalized_action = normalize_actions(action)\n",
    "print(\"Normalized Action:\", normalized_action)\n",
    "\n",
    "# Denormalize the action\n",
    "denormalized_action = denormalize_actions(normalized_action)\n",
    "print(\"Denormalized Action:\", denormalized_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load observation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/rollout/dataset_clamped.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m expert_dataset \u001b[38;5;241m=\u001b[39m ExpertDataSet()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mexpert_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/rollout/dataset_clamped.npz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m observation shape : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpert_dataset\u001b[38;5;241m.\u001b[39mobservations\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m action shape : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpert_dataset\u001b[38;5;241m.\u001b[39mactions\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\Memoire-loic-RL\\gym-donkeycar\\CarConsumptionModel\\utils\\ExpertDataset.py:33\u001b[0m, in \u001b[0;36mExpertDataSet.load_dataset\u001b[1;34m(self, file_path)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_dataset\u001b[39m(\u001b[38;5;28mself\u001b[39m, file_path : \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/rollout/dataset.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m data:\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservations \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\openhub\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/rollout/dataset_clamped.npz'"
     ]
    }
   ],
   "source": [
    "expert_dataset = ExpertDataSet()\n",
    "expert_dataset.load_dataset(\"../data/rollout/dataset_clamped.npz\")\n",
    "\n",
    "print(f\" observation shape : {expert_dataset.observations.shape} \\n\")\n",
    "print(f\" action shape : {expert_dataset.actions.shape} \\n\")\n",
    "\n",
    "def transform_observation(obs: np.ndarray) -> np.ndarray:\n",
    "    # transform (1332, 120, 160, 3) to (1332, 3, 120, 160)\n",
    "    transformed_obs = np.transpose(obs, (0, 3, 1, 2))\n",
    "    return transformed_obs\n",
    "\n",
    "expert_dataset.observations = transform_observation(expert_dataset.observations)\n",
    "train_size = int(0.8 * len(expert_dataset))\n",
    "\n",
    "test_size = len(expert_dataset) - train_size\n",
    "\n",
    "train_expert_dataset, test_expert_dataset = random_split(\n",
    "    expert_dataset, [train_size, test_size]\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size: {train_size} \\n\"\n",
    "        f\"Test dataset size: {test_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\spige\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n",
      "c:\\Users\\spige\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:231: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 11.52GB > 4.04GB\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# create DDPG model with replay buffer of 100k\n",
    "clone_ppo_model = DDPG(\"CnnPolicy\", env, verbose=1, seed=42, buffer_size=100000)\n",
    "EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_agent(\n",
    "    student,\n",
    "    batch_size=64,\n",
    "    epochs=1000,\n",
    "    scheduler_gamma=0.7,\n",
    "    learning_rate=1.0,\n",
    "    log_interval=100,\n",
    "    no_cuda=True,\n",
    "    seed=1,\n",
    "    test_batch_size=64,\n",
    "    csv_file=\"../data/RL/pretraining.csv\",\n",
    "    test_file=\"../data/RL/pretraining_test.csv\",\n",
    "    loss_file=\"../data/RL/pretraining_loss.csv\",\n",
    "    test_loss_file=\"../data/RL/pretraining_test_loss.csv\",\n",
    "    ppo_model=clone_ppo_model,\n",
    "):\n",
    "    use_cuda = not no_cuda and th.cuda.is_available()\n",
    "    th.manual_seed(seed)\n",
    "    device = th.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    kwargs = {\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {}\n",
    "    print(f\"Using device {device}\") \n",
    "    \n",
    "    criterion = nn.L1Loss()\n",
    "    #criterion = nn.MSELoss()\n",
    "\n",
    "    # Extract initial policy\n",
    "    model = student.policy.to(device)\n",
    "\n",
    "    file = open(csv_file, \"w\")\n",
    "    file.write(\"observed_steering,observed_throttle,target_steering,target_throttle\\n\")\n",
    "\n",
    "    test_file = open(test_file, \"w\")\n",
    "    test_file.write(\"observed_steering,observed_throttle,target_steering,target_throttle\\n\")\n",
    "\n",
    "    file_loss = open(loss_file, \"w\")\n",
    "    file_loss.write(\"loss\\n\")\n",
    "\n",
    "    test_file_loss = open(test_loss_file, \"w\")\n",
    "    test_file_loss.write(\"loss\\n\")\n",
    "\n",
    "    def train(model, device, train_loader, optimizer):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (current_observation, target_action) in enumerate(train_loader):\n",
    "            current_observation, target_action = current_observation.to(device), target_action.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if isinstance(env.action_space, gym.spaces.Box):\n",
    "                # A2C/PPO policy outputs actions, values, log_prob\n",
    "                # SAC/TD3 policy outputs actions only\n",
    "                if isinstance(student, (A2C, PPO)):\n",
    "                    action, _, _ = model(current_observation)\n",
    "                else:\n",
    "                    # SAC/TD3:\n",
    "                    action = model(current_observation)\n",
    "                action_prediction = action.double()\n",
    "            else:\n",
    "                # Retrieve the logits for A2C/PPO when using discrete actions\n",
    "                dist = model.get_distribution(current_observation)\n",
    "                action_prediction = dist.distribution.logits\n",
    "                target_action = target_action.long()\n",
    "\n",
    "            loss = criterion(action_prediction, target_action)\n",
    "            file_loss.write(f\"{loss.item()}\\n\")\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            for observed_action, target in zip(action_prediction, target_action):\n",
    "                # extract numpy scalars from PyTorch tensors\n",
    "                observed_action = observed_action.cpu().detach().numpy()\n",
    "                target = target.cpu().detach().numpy()\n",
    "\n",
    "                observed_action = \",\".join(map(str, observed_action))\n",
    "                target = \",\".join(map(str, target))\n",
    "                \n",
    "                file.write(f\"{observed_action},{target}\\n\")\n",
    "\n",
    "            \n",
    "            if batch_idx % log_interval == 0:\n",
    "                print(\n",
    "                    \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                        epoch,\n",
    "                        batch_idx * len(current_observation),\n",
    "                        len(train_loader.dataset),\n",
    "                        100.0 * batch_idx / len(train_loader),\n",
    "                        loss.item(),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    def test(model, device, test_loader):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with th.no_grad():\n",
    "            for current_observation, target_action in test_loader:\n",
    "                current_observation, target_action = current_observation.to(device), target_action.to(device)\n",
    "\n",
    "                if isinstance(env.action_space, gym.spaces.Box):\n",
    "                    # A2C/PPO policy outputs actions, values, log_prob\n",
    "                    # SAC/TD3 policy outputs actions only\n",
    "                    if isinstance(student, (A2C, PPO)):\n",
    "                        action, _, _ = model(current_observation)\n",
    "                    else:\n",
    "                        # SAC/TD3:\n",
    "                        action = model(current_observation)\n",
    "                    action_prediction = action.double()\n",
    "                else:\n",
    "                    # Retrieve the logits for A2C/PPO when using discrete actions\n",
    "                    dist = model.get_distribution(current_observation)\n",
    "                    action_prediction = dist.distribution.logits\n",
    "                    target_action = target_action.long()\n",
    "                \n",
    "                test_loss = criterion(action_prediction, target_action)\n",
    "                for observed_action, target in zip(action_prediction, target_action):\n",
    "                    # extract numpy scalars from PyTorch tensors\n",
    "                    observed_action = observed_action.cpu().detach().numpy()\n",
    "                    target = target.cpu().detach().numpy()\n",
    "\n",
    "                    observed_action = \",\".join(map(str, observed_action))\n",
    "                    target = \",\".join(map(str, target))\n",
    "                    \n",
    "                    test_file.write(f\"{observed_action},{target}\\n\")\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print(f\"Test set: Average loss: {test_loss}\")\n",
    "        test_file_loss.write(f\"{test_loss}\\n\")\n",
    "\n",
    "    # Here, we use PyTorch `DataLoader` to our load previously created `ExpertDataset` for training\n",
    "    # and testing\n",
    "\n",
    "    train_loader = th.utils.data.DataLoader(\n",
    "        dataset=train_expert_dataset, batch_size=batch_size, shuffle=True, **kwargs\n",
    "    )\n",
    "    test_loader = th.utils.data.DataLoader(\n",
    "        dataset=test_expert_dataset,\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=True,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    # Define an Optimizer and a learning rate schedule.\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=scheduler_gamma)\n",
    "\n",
    "    # Now we are finally ready to train the policy model.\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer)\n",
    "        test(model, device, test_loader)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        model_name = clone_ppo_model.__class__.__name__\n",
    "        # save the model policy\n",
    "        th.save(model.state_dict(), f\"../models/pretrained_{model_name}_{epoch}.pt\")\n",
    "        ppo_model.policy.load_state_dict(model.state_dict())    \n",
    "\n",
    "        ppo_model.save(f\"../models/pretrained_{model_name}_{epoch}\")\n",
    "\n",
    "    file.close()\n",
    "    file_loss.close()\n",
    "    test_file_loss.close()\n",
    "\n",
    "    # Implant the trained policy network back into the RL student agent\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n",
      "Train Epoch: 1 [0/154850 (0%)]\tLoss: 0.168921\n",
      "Train Epoch: 1 [6400/154850 (4%)]\tLoss: 0.091467\n",
      "Train Epoch: 1 [12800/154850 (8%)]\tLoss: 0.061559\n",
      "Train Epoch: 1 [19200/154850 (12%)]\tLoss: 0.052992\n",
      "Train Epoch: 1 [25600/154850 (17%)]\tLoss: 0.055536\n",
      "Train Epoch: 1 [32000/154850 (21%)]\tLoss: 0.044700\n",
      "Train Epoch: 1 [38400/154850 (25%)]\tLoss: 0.035171\n",
      "Train Epoch: 1 [44800/154850 (29%)]\tLoss: 0.037085\n",
      "Train Epoch: 1 [51200/154850 (33%)]\tLoss: 0.036672\n",
      "Train Epoch: 1 [57600/154850 (37%)]\tLoss: 0.035680\n",
      "Train Epoch: 1 [64000/154850 (41%)]\tLoss: 0.037397\n",
      "Train Epoch: 1 [70400/154850 (45%)]\tLoss: 0.040269\n",
      "Train Epoch: 1 [76800/154850 (50%)]\tLoss: 0.035380\n",
      "Train Epoch: 1 [83200/154850 (54%)]\tLoss: 0.032975\n",
      "Train Epoch: 1 [89600/154850 (58%)]\tLoss: 0.036750\n",
      "Train Epoch: 1 [96000/154850 (62%)]\tLoss: 0.038472\n",
      "Train Epoch: 1 [102400/154850 (66%)]\tLoss: 0.034886\n",
      "Train Epoch: 1 [108800/154850 (70%)]\tLoss: 0.029305\n",
      "Train Epoch: 1 [115200/154850 (74%)]\tLoss: 0.037246\n",
      "Train Epoch: 1 [121600/154850 (79%)]\tLoss: 0.031696\n",
      "Train Epoch: 1 [128000/154850 (83%)]\tLoss: 0.035414\n",
      "Train Epoch: 1 [134400/154850 (87%)]\tLoss: 0.034411\n",
      "Train Epoch: 1 [140800/154850 (91%)]\tLoss: 0.034578\n",
      "Train Epoch: 1 [147200/154850 (95%)]\tLoss: 0.033695\n",
      "Train Epoch: 1 [153600/154850 (99%)]\tLoss: 0.028754\n",
      "Test set: Average loss: 7.054574133729092e-07\n",
      "Train Epoch: 2 [0/154850 (0%)]\tLoss: 0.029551\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m name \u001b[38;5;241m=\u001b[39m clone_ppo_model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m      7\u001b[0m directory \u001b[38;5;241m=\u001b[39m directory \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m policy_model \u001b[38;5;241m=\u001b[39m \u001b[43mpretrain_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstudent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mclone_ppo_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_gamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_cuda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mppo_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclone_ppo_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirectory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpretraining.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirectory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpretraining_test.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirectory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpretraining_loss.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loss_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirectory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpretraining_test_loss.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     24\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 145\u001b[0m, in \u001b[0;36mpretrain_agent\u001b[1;34m(student, batch_size, epochs, scheduler_gamma, learning_rate, log_interval, no_cuda, seed, test_batch_size, csv_file, test_file, loss_file, test_loss_file, ppo_model)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Now we are finally ready to train the policy model.\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 145\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m     test(model, device, test_loader)\n\u001b[0;32m    148\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[7], line 66\u001b[0m, in \u001b[0;36mpretrain_agent.<locals>.train\u001b[1;34m(model, device, train_loader, optimizer)\u001b[0m\n\u001b[0;32m     64\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(action_prediction, target_action)\n\u001b[0;32m     65\u001b[0m file_loss\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 66\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m observed_action, target \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(action_prediction, target_action):\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# extract numpy scalars from PyTorch tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\spige\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\spige\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "directory = \"../data/RL/\"\n",
    "\n",
    "# get name of \"clone_ppo_model\" instance\n",
    "\n",
    "name = clone_ppo_model.__class__.__name__\n",
    "\n",
    "directory = directory + name + \"_\"\n",
    "\n",
    "policy_model = pretrain_agent(\n",
    "    student = clone_ppo_model,\n",
    "    epochs = EPOCHS,\n",
    "    scheduler_gamma=0.7,\n",
    "    learning_rate=1.0,\n",
    "    log_interval=100,\n",
    "    no_cuda=True,\n",
    "    seed=1,\n",
    "    batch_size=64,\n",
    "    test_batch_size=64,\n",
    "    ppo_model=clone_ppo_model,\n",
    "    csv_file=directory + \"pretraining.csv\",\n",
    "    test_file=directory + \"pretraining_test.csv\",\n",
    "    loss_file=directory + \"pretraining_loss.csv\",\n",
    "    test_loss_file=directory + \"pretraining_test_loss.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.  0.]\n",
      "[1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space.low)\n",
    "print(env.action_space.high)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
