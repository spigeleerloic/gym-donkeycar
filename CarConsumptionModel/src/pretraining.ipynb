{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from stable_baselines3 import PPO, SAC, TD3, DDPG, A2C, DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from gym_donkeycar.envs.donkey_env import DonkeyEnv\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, CallbackList\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "\n",
    "from sb3_contrib import TQC\n",
    "\n",
    "import sys\n",
    "\n",
    "from torch.utils.data.dataset import Dataset, random_split\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import gym\n",
    "\n",
    "path = r\"c:\\Users\\spige\\memoire\\gym-donkeycar-retry\\gym-donkeycar\\CarConsumptionModel\"\n",
    "sys.path.insert(0, path)\n",
    "\n",
    "\n",
    "from donkey_environment.ConsumptionWrapper import ConsumptionWrapper\n",
    "from utils.callbacks import CustomProgressBarCallback\n",
    "from utils.ExpertDataset import ExpertDataSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.8\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#! pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# print(th.version.cuda)\n",
    "# print(th.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creation of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting DonkeyGym env\n",
      "Setting default: start_delay 0.0\n",
      "Setting default: max_cte 8.0\n",
      "Setting default: frame_skip 1\n",
      "Setting default: cam_resolution (120, 160, 3)\n",
      "Setting default: log_level 20\n",
      "Setting default: host localhost\n",
      "Setting default: port 9091\n",
      "Setting default: steer_limit 1.0\n",
      "{'throttle_min': 0.0, 'throttle_max': 1.0, 'level': 'steep-ascent', 'start_delay': 0.0, 'max_cte': 8.0, 'frame_skip': 1, 'cam_resolution': (120, 160, 3), 'log_level': 20, 'host': 'localhost', 'port': 9091, 'steer_limit': 1.0}\n",
      "loading scene steep-ascent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\spige\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\spaces\\box.py:78: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "c:\\Users\\spige\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "env_kwargs = {\n",
    "    \"level\": \"steep-ascent\",\n",
    "}\n",
    "\n",
    "conf = {\n",
    "    \"throttle_min\" :  0.0,\n",
    "    \"throttle_max\" : 1.0,\n",
    "}\n",
    "env_kwargs.update({\"conf\": conf})\n",
    "\n",
    "env = make_vec_env(\n",
    "    ConsumptionWrapper, \n",
    "    n_envs=1, \n",
    "    env_kwargs=env_kwargs, \n",
    "    seed=42,\n",
    "    vec_env_cls=DummyVecEnv,\n",
    ")\n",
    "#env = ConsumptionWrapper(level=\"steep-ascent\", conf={\"throttle_min\": 0.0, \"throttle_max\": 1.0})\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalization methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Action: tensor([0.5000, 0.5000], dtype=torch.float64)\n",
      "Denormalized Action: tensor([0.5000, 0.5000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def normalize_actions(action: np.ndarray) -> np.ndarray:\n",
    "     # [-1, -1], [1,1]\n",
    "    \n",
    "    low, high = np.array([-1, -1]), np.array([1, 1])\n",
    "    predicted_action = 2 * (action - low) / (high - low) - 1\n",
    "    return predicted_action\n",
    "\n",
    "def denormalize_actions(normalized_action: np.ndarray) -> np.ndarray:\n",
    "    low, high = np.array([-1, -1]), np.array([1, 1])\n",
    "    denormalized_action = (normalized_action + 1) / 2 * (high - low) + low\n",
    "    return denormalized_action\n",
    "\n",
    "\n",
    "# Example action\n",
    "action = np.array([0.5, 0.5])\n",
    "# convert to tensor\n",
    "action = th.tensor(action)\n",
    "\n",
    "# Normalize the action\n",
    "normalized_action = normalize_actions(action)\n",
    "print(\"Normalized Action:\", normalized_action)\n",
    "\n",
    "# Denormalize the action\n",
    "denormalized_action = denormalize_actions(normalized_action)\n",
    "print(\"Denormalized Action:\", denormalized_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load observation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_dataset = ExpertDataSet()\n",
    "expert_dataset.load_dataset(\"../data/rollout/dataset_clamped.npz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " observation shape : (193563, 120, 160, 3) \n",
      "\n",
      " action shape : (193563, 2) \n",
      "\n",
      "Train dataset size: 154850 \n",
      "Test dataset size: 38713\n"
     ]
    }
   ],
   "source": [
    "print(f\" observation shape : {expert_dataset.observations.shape} \\n\")\n",
    "print(f\" action shape : {expert_dataset.actions.shape} \\n\")\n",
    "\n",
    "def transform_observation(obs: np.ndarray) -> np.ndarray:\n",
    "    # transform (1332, 120, 160, 3) to (1332, 3, 120, 160)\n",
    "    transformed_obs = np.transpose(obs, (0, 3, 1, 2))\n",
    "    return transformed_obs\n",
    "\n",
    "expert_dataset.observations = transform_observation(expert_dataset.observations)\n",
    "train_size = int(0.8 * len(expert_dataset))\n",
    "\n",
    "test_size = len(expert_dataset) - train_size\n",
    "\n",
    "train_expert_dataset, test_expert_dataset = random_split(\n",
    "    expert_dataset, [train_size, test_size]\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size: {train_size} \\n\"\n",
    "        f\"Test dataset size: {test_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "# create DDPG model with replay buffer of 100k\n",
    "clone_ppo_model = PPO(\"CnnPolicy\", env, verbose=1, seed=42)\n",
    "EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_agent(\n",
    "    student,\n",
    "    batch_size=64,\n",
    "    epochs=1000,\n",
    "    scheduler_gamma=0.7,\n",
    "    learning_rate=1.0,\n",
    "    log_interval=100,\n",
    "    no_cuda=True,\n",
    "    seed=1,\n",
    "    test_batch_size=64,\n",
    "    csv_file=\"../data/RL/pretraining.csv\",\n",
    "    test_file=\"../data/RL/pretraining_test.csv\",\n",
    "    loss_file=\"../data/RL/pretraining_loss.csv\",\n",
    "    test_loss_file=\"../data/RL/pretraining_test_loss.csv\",\n",
    "    ppo_model=clone_ppo_model,\n",
    "):\n",
    "    use_cuda = not no_cuda and th.cuda.is_available()\n",
    "    th.manual_seed(seed)\n",
    "    device = th.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    kwargs = {\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {}\n",
    "    print(f\"Using device {device}\") \n",
    "    \n",
    "    criterion = nn.L1Loss()\n",
    "    #criterion = nn.MSELoss()\n",
    "\n",
    "    # Extract initial policy\n",
    "    model = student.policy.to(device)\n",
    "\n",
    "    file = open(csv_file, \"w\")\n",
    "    file.write(\"observed_steering,observed_throttle,target_steering,target_throttle\\n\")\n",
    "\n",
    "    test_file = open(test_file, \"w\")\n",
    "    test_file.write(\"observed_steering,observed_throttle,target_steering,target_throttle\\n\")\n",
    "\n",
    "    file_loss = open(loss_file, \"w\")\n",
    "    file_loss.write(\"loss\\n\")\n",
    "\n",
    "    test_file_loss = open(test_loss_file, \"w\")\n",
    "    test_file_loss.write(\"loss\\n\")\n",
    "\n",
    "    def train(model, device, train_loader, optimizer):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (current_observation, target_action) in enumerate(train_loader):\n",
    "            current_observation, target_action = current_observation.to(device), target_action.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            action, _, _ = model(current_observation)\n",
    "            action_prediction = action.double()\n",
    "\n",
    "            loss = criterion(action_prediction, target_action)\n",
    "            file_loss.write(f\"{loss.item()}\\n\")\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            for observed_action, target in zip(action_prediction, target_action):\n",
    "                # extract numpy scalars from PyTorch tensors\n",
    "                observed_action = observed_action.cpu().detach().numpy()\n",
    "                target = target.cpu().detach().numpy()\n",
    "\n",
    "                observed_action = \",\".join(map(str, observed_action))\n",
    "                target = \",\".join(map(str, target))\n",
    "                \n",
    "                file.write(f\"{observed_action},{target}\\n\")\n",
    "\n",
    "            \n",
    "            if batch_idx % log_interval == 0:\n",
    "                print(\n",
    "                    \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                        epoch,\n",
    "                        batch_idx * len(current_observation),\n",
    "                        len(train_loader.dataset),\n",
    "                        100.0 * batch_idx / len(train_loader),\n",
    "                        loss.item(),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    def test(model, device, test_loader):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with th.no_grad():\n",
    "            for current_observation, target_action in test_loader:\n",
    "                current_observation, target_action = current_observation.to(device), target_action.to(device)\n",
    "\n",
    "                action, _, _ = model(current_observation)\n",
    "                action_prediction = action.double()\n",
    "                \n",
    "                test_loss = criterion(action_prediction, target_action)\n",
    "                for observed_action, target in zip(action_prediction, target_action):\n",
    "                    # extract numpy scalars from PyTorch tensors\n",
    "                    observed_action = observed_action.cpu().detach().numpy()\n",
    "                    target = target.cpu().detach().numpy()\n",
    "\n",
    "                    observed_action = \",\".join(map(str, observed_action))\n",
    "                    target = \",\".join(map(str, target))\n",
    "                    \n",
    "                    test_file.write(f\"{observed_action},{target}\\n\")\n",
    "                #test_loss /= len(test_loader.dataset)\n",
    "                print(f\"Test set: Average loss: {test_loss}\")\n",
    "                test_file_loss.write(f\"{test_loss}\\n\")\n",
    "\n",
    "    # Here, we use PyTorch `DataLoader` to our load previously created `ExpertDataset` for training\n",
    "    # and testing\n",
    "\n",
    "    train_loader = th.utils.data.DataLoader(\n",
    "        dataset=train_expert_dataset, batch_size=batch_size, shuffle=True, **kwargs\n",
    "    )\n",
    "    test_loader = th.utils.data.DataLoader(\n",
    "        dataset=test_expert_dataset,\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=True,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    # Define an Optimizer and a learning rate schedule.\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=scheduler_gamma)\n",
    "\n",
    "    # Now we are finally ready to train the policy model.\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer)\n",
    "        test(model, device, test_loader)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        model_name = clone_ppo_model.__class__.__name__\n",
    "        # save the model policy\n",
    "        th.save(model.state_dict(), f\"../models/pretrained_{model_name}_{epoch}.pt\")\n",
    "        ppo_model.policy.load_state_dict(model.state_dict())    \n",
    "\n",
    "        ppo_model.save(f\"../models/pretrained_{model_name}_{epoch}\")\n",
    "\n",
    "    file.close()\n",
    "    file_loss.close()\n",
    "    test_file_loss.close()\n",
    "\n",
    "    # Implant the trained policy network back into the RL student agent\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n",
      "Train Epoch: 1 [0/154850 (0%)]\tLoss: 0.879044\n",
      "Train Epoch: 1 [6400/154850 (4%)]\tLoss: 0.666611\n",
      "Train Epoch: 1 [12800/154850 (8%)]\tLoss: 0.419100\n",
      "Train Epoch: 1 [19200/154850 (12%)]\tLoss: 0.294111\n",
      "Train Epoch: 1 [25600/154850 (17%)]\tLoss: 0.221889\n",
      "Train Epoch: 1 [32000/154850 (21%)]\tLoss: 0.159708\n",
      "Train Epoch: 1 [38400/154850 (25%)]\tLoss: 0.094768\n",
      "Train Epoch: 1 [44800/154850 (29%)]\tLoss: 0.081478\n",
      "Train Epoch: 1 [51200/154850 (33%)]\tLoss: 0.072974\n",
      "Train Epoch: 1 [57600/154850 (37%)]\tLoss: 0.066448\n",
      "Train Epoch: 1 [64000/154850 (41%)]\tLoss: 0.054528\n",
      "Train Epoch: 1 [70400/154850 (45%)]\tLoss: 0.049485\n",
      "Train Epoch: 1 [76800/154850 (50%)]\tLoss: 0.044081\n",
      "Train Epoch: 1 [83200/154850 (54%)]\tLoss: 0.041010\n",
      "Train Epoch: 1 [89600/154850 (58%)]\tLoss: 0.045359\n",
      "Train Epoch: 1 [96000/154850 (62%)]\tLoss: 0.051094\n",
      "Train Epoch: 1 [102400/154850 (66%)]\tLoss: 0.041788\n",
      "Train Epoch: 1 [108800/154850 (70%)]\tLoss: 0.036258\n",
      "Train Epoch: 1 [115200/154850 (74%)]\tLoss: 0.032814\n",
      "Train Epoch: 1 [121600/154850 (79%)]\tLoss: 0.032206\n",
      "Train Epoch: 1 [128000/154850 (83%)]\tLoss: 0.038866\n",
      "Train Epoch: 1 [134400/154850 (87%)]\tLoss: 0.038670\n",
      "Train Epoch: 1 [140800/154850 (91%)]\tLoss: 0.034173\n",
      "Train Epoch: 1 [147200/154850 (95%)]\tLoss: 0.031490\n",
      "Train Epoch: 1 [153600/154850 (99%)]\tLoss: 0.032844\n",
      "Test set: Average loss: 0.029581721298200137\n",
      "Test set: Average loss: 0.0356941995141824\n",
      "Test set: Average loss: 0.03377204846310633\n",
      "Test set: Average loss: 0.03550498745494224\n",
      "Test set: Average loss: 0.03790847040727385\n",
      "Test set: Average loss: 0.027834146265377058\n",
      "Test set: Average loss: 0.031487888543324516\n",
      "Test set: Average loss: 0.037702700577028736\n",
      "Test set: Average loss: 0.030451085922777565\n",
      "Test set: Average loss: 0.02995624345135184\n",
      "Test set: Average loss: 0.03230886228266172\n",
      "Test set: Average loss: 0.037503472102798696\n",
      "Test set: Average loss: 0.03363696025189711\n",
      "Test set: Average loss: 0.03623917838285706\n",
      "Test set: Average loss: 0.037389795067610976\n",
      "Test set: Average loss: 0.03238815420945684\n",
      "Test set: Average loss: 0.028924794529302744\n",
      "Test set: Average loss: 0.03234944316500332\n",
      "Test set: Average loss: 0.03672142739287665\n",
      "Test set: Average loss: 0.03031742821985972\n",
      "Test set: Average loss: 0.03029860370043025\n",
      "Test set: Average loss: 0.03311798678714695\n",
      "Test set: Average loss: 0.033761610195142566\n",
      "Test set: Average loss: 0.03527270651647996\n",
      "Test set: Average loss: 0.03318672220316898\n",
      "Test set: Average loss: 0.029735278844782442\n",
      "Test set: Average loss: 0.02871375649192487\n",
      "Test set: Average loss: 0.03466191109691863\n",
      "Test set: Average loss: 0.03165178419885706\n",
      "Test set: Average loss: 0.03586268768958689\n",
      "Test set: Average loss: 0.03245773648995964\n",
      "Test set: Average loss: 0.03419228101711269\n",
      "Test set: Average loss: 0.033760823656848515\n",
      "Test set: Average loss: 0.03229362097295052\n",
      "Test set: Average loss: 0.03609057384915104\n",
      "Test set: Average loss: 0.03585785182804102\n",
      "Test set: Average loss: 0.03610321203723288\n",
      "Test set: Average loss: 0.02814568475150736\n",
      "Test set: Average loss: 0.03303139072238537\n",
      "Test set: Average loss: 0.03366207661383669\n",
      "Test set: Average loss: 0.03626008011815429\n",
      "Test set: Average loss: 0.036992561752867914\n",
      "Test set: Average loss: 0.04007165597704443\n",
      "Test set: Average loss: 0.03488463618487003\n",
      "Test set: Average loss: 0.030405871894572556\n",
      "Test set: Average loss: 0.03558901508040435\n",
      "Test set: Average loss: 0.03519511825834343\n",
      "Test set: Average loss: 0.035613619544164976\n",
      "Test set: Average loss: 0.0359187747822034\n",
      "Test set: Average loss: 0.030987418969743885\n",
      "Test set: Average loss: 0.029079548636900654\n",
      "Test set: Average loss: 0.027298298310597602\n",
      "Test set: Average loss: 0.030436822671617847\n",
      "Test set: Average loss: 0.031605923639745015\n",
      "Test set: Average loss: 0.032138846754605765\n",
      "Test set: Average loss: 0.034089705685801164\n",
      "Test set: Average loss: 0.032879400686397275\n",
      "Test set: Average loss: 0.03344425068098644\n",
      "Test set: Average loss: 0.03310778302034123\n",
      "Test set: Average loss: 0.03139496158499355\n",
      "Test set: Average loss: 0.034909381694888\n",
      "Test set: Average loss: 0.03201675398850057\n",
      "Test set: Average loss: 0.03592679060238879\n",
      "Test set: Average loss: 0.0455332072397141\n",
      "Test set: Average loss: 0.03885805427307787\n",
      "Test set: Average loss: 0.03046284043830383\n",
      "Test set: Average loss: 0.03726830173354756\n",
      "Test set: Average loss: 0.030646992717265675\n",
      "Test set: Average loss: 0.027832473698254034\n",
      "Test set: Average loss: 0.033114868334450875\n",
      "Test set: Average loss: 0.03387363775254926\n",
      "Test set: Average loss: 0.03399647869809996\n",
      "Test set: Average loss: 0.03822309527458856\n",
      "Test set: Average loss: 0.03658763273188015\n",
      "Test set: Average loss: 0.03149628351911815\n",
      "Test set: Average loss: 0.043562515082157915\n",
      "Test set: Average loss: 0.033527346359960575\n",
      "Test set: Average loss: 0.03435840528982226\n",
      "Test set: Average loss: 0.035089880789200834\n",
      "Test set: Average loss: 0.03447978041822353\n",
      "Test set: Average loss: 0.03561793708104233\n",
      "Test set: Average loss: 0.030819302279269323\n",
      "Test set: Average loss: 0.035591652652328776\n",
      "Test set: Average loss: 0.0372063393948423\n",
      "Test set: Average loss: 0.03958252453912792\n",
      "Test set: Average loss: 0.04261538347691385\n",
      "Test set: Average loss: 0.036090676951971545\n",
      "Test set: Average loss: 0.0348366630187229\n",
      "Test set: Average loss: 0.0324457221204284\n",
      "Test set: Average loss: 0.03259385490309796\n",
      "Test set: Average loss: 0.036470446349994745\n",
      "Test set: Average loss: 0.034911930187490725\n",
      "Test set: Average loss: 0.031874351434453274\n",
      "Test set: Average loss: 0.0366206964426965\n",
      "Test set: Average loss: 0.03280097950482741\n",
      "Test set: Average loss: 0.03862390597896592\n",
      "Test set: Average loss: 0.039138946768161986\n",
      "Test set: Average loss: 0.031163468684098916\n",
      "Test set: Average loss: 0.027725863837780196\n",
      "Test set: Average loss: 0.032129046696354635\n",
      "Test set: Average loss: 0.030384242833861208\n",
      "Test set: Average loss: 0.039717696785373846\n",
      "Test set: Average loss: 0.032144993007023004\n",
      "Test set: Average loss: 0.03690556809095824\n",
      "Test set: Average loss: 0.034776027878251625\n",
      "Test set: Average loss: 0.03224209119707666\n",
      "Test set: Average loss: 0.03666111841585007\n",
      "Test set: Average loss: 0.033077892271649034\n",
      "Test set: Average loss: 0.028326579457825574\n",
      "Test set: Average loss: 0.02788607823822531\n",
      "Test set: Average loss: 0.031929026925354265\n",
      "Test set: Average loss: 0.034865148472817964\n",
      "Test set: Average loss: 0.04945108315314428\n",
      "Test set: Average loss: 0.0307548441369363\n",
      "Test set: Average loss: 0.03815023206061596\n",
      "Test set: Average loss: 0.03419248828458876\n",
      "Test set: Average loss: 0.03048655678276191\n",
      "Test set: Average loss: 0.026424701678479323\n",
      "Test set: Average loss: 0.032495906969415955\n",
      "Test set: Average loss: 0.0391214403422282\n",
      "Test set: Average loss: 0.037604356749397994\n",
      "Test set: Average loss: 0.03177544792788467\n",
      "Test set: Average loss: 0.03241460082222147\n",
      "Test set: Average loss: 0.03525466741484706\n",
      "Test set: Average loss: 0.02962003301945515\n",
      "Test set: Average loss: 0.03725516929807782\n",
      "Test set: Average loss: 0.027195535725695663\n",
      "Test set: Average loss: 0.040164322692362475\n",
      "Test set: Average loss: 0.034579645408484794\n",
      "Test set: Average loss: 0.03487436977138714\n",
      "Test set: Average loss: 0.0358371779184381\n",
      "Test set: Average loss: 0.03356673259258969\n",
      "Test set: Average loss: 0.028520555191789754\n",
      "Test set: Average loss: 0.03238826862980204\n",
      "Test set: Average loss: 0.0338281097343156\n",
      "Test set: Average loss: 0.03501379666340654\n",
      "Test set: Average loss: 0.0350639516764204\n",
      "Test set: Average loss: 0.03575696379994042\n",
      "Test set: Average loss: 0.03549936245963181\n",
      "Test set: Average loss: 0.04429436545342469\n",
      "Test set: Average loss: 0.027808861185803835\n",
      "Test set: Average loss: 0.04083298574005312\n",
      "Test set: Average loss: 0.03277312486352457\n",
      "Test set: Average loss: 0.0357755540189828\n",
      "Test set: Average loss: 0.03213589451479493\n",
      "Test set: Average loss: 0.03573624677301268\n",
      "Test set: Average loss: 0.03254357402693131\n",
      "Test set: Average loss: 0.035723983733532805\n",
      "Test set: Average loss: 0.04346677728972281\n",
      "Test set: Average loss: 0.031227304137246392\n",
      "Test set: Average loss: 0.037462315421407766\n",
      "Test set: Average loss: 0.0382107040018127\n",
      "Test set: Average loss: 0.03528657058814133\n",
      "Test set: Average loss: 0.04104342839400488\n",
      "Test set: Average loss: 0.033684971915135975\n",
      "Test set: Average loss: 0.033258246955938375\n",
      "Test set: Average loss: 0.035187884019251214\n",
      "Test set: Average loss: 0.03597516384161281\n",
      "Test set: Average loss: 0.03145741475213981\n",
      "Test set: Average loss: 0.033782277865839205\n",
      "Test set: Average loss: 0.033663908398921194\n",
      "Test set: Average loss: 0.03368782417419425\n",
      "Test set: Average loss: 0.03622353374601062\n",
      "Test set: Average loss: 0.03484583268073038\n",
      "Test set: Average loss: 0.045100907084815844\n",
      "Test set: Average loss: 0.03127226936794614\n",
      "Test set: Average loss: 0.031849568578763865\n",
      "Test set: Average loss: 0.033377493285115634\n",
      "Test set: Average loss: 0.03622612120125268\n",
      "Test set: Average loss: 0.02960491340718363\n",
      "Test set: Average loss: 0.030798001878793002\n",
      "Test set: Average loss: 0.034049170461003087\n",
      "Test set: Average loss: 0.04503185470821336\n",
      "Test set: Average loss: 0.032943991031970654\n",
      "Test set: Average loss: 0.0384693655087176\n",
      "Test set: Average loss: 0.025108008996539866\n",
      "Test set: Average loss: 0.03113017068812951\n",
      "Test set: Average loss: 0.030266714952631446\n",
      "Test set: Average loss: 0.030575169921576162\n",
      "Test set: Average loss: 0.031673189312641625\n",
      "Test set: Average loss: 0.03484453135752119\n",
      "Test set: Average loss: 0.03665899283532781\n",
      "Test set: Average loss: 0.028125507265031047\n",
      "Test set: Average loss: 0.0317641747315065\n",
      "Test set: Average loss: 0.031389177676828695\n",
      "Test set: Average loss: 0.03062337504752577\n",
      "Test set: Average loss: 0.03030678408595122\n",
      "Test set: Average loss: 0.03134241253701475\n",
      "Test set: Average loss: 0.03220187034457922\n",
      "Test set: Average loss: 0.03246168748864875\n",
      "Test set: Average loss: 0.034067853819578886\n",
      "Test set: Average loss: 0.03557749294054702\n",
      "Test set: Average loss: 0.03502987046340422\n",
      "Test set: Average loss: 0.036153997890778555\n",
      "Test set: Average loss: 0.0367744171362574\n",
      "Test set: Average loss: 0.03832444912131905\n",
      "Test set: Average loss: 0.041106885806584614\n",
      "Test set: Average loss: 0.034400325570459245\n",
      "Test set: Average loss: 0.030322145684522184\n",
      "Test set: Average loss: 0.03508544303349481\n",
      "Test set: Average loss: 0.03457541337411385\n",
      "Test set: Average loss: 0.032769694460967\n",
      "Test set: Average loss: 0.039480494713643566\n",
      "Test set: Average loss: 0.02779988350266649\n",
      "Test set: Average loss: 0.026593750959364115\n",
      "Test set: Average loss: 0.028971836848995736\n",
      "Test set: Average loss: 0.027032281303036143\n",
      "Test set: Average loss: 0.03192482399367691\n",
      "Test set: Average loss: 0.03216535608498816\n",
      "Test set: Average loss: 0.03158507636499053\n",
      "Test set: Average loss: 0.0319899256055578\n",
      "Test set: Average loss: 0.03529109304326994\n",
      "Test set: Average loss: 0.03277953750875895\n",
      "Test set: Average loss: 0.031342112429683766\n",
      "Test set: Average loss: 0.03859963173908909\n",
      "Test set: Average loss: 0.028311949403359904\n",
      "Test set: Average loss: 0.03411011814387166\n",
      "Test set: Average loss: 0.030281166838904028\n",
      "Test set: Average loss: 0.03281928286378388\n",
      "Test set: Average loss: 0.034105158824786486\n",
      "Test set: Average loss: 0.03852148452097026\n",
      "Test set: Average loss: 0.03327441441706469\n",
      "Test set: Average loss: 0.03211630366604368\n",
      "Test set: Average loss: 0.03343086481163482\n",
      "Test set: Average loss: 0.03258629878382635\n",
      "Test set: Average loss: 0.0348673899025016\n",
      "Test set: Average loss: 0.028616025929522948\n",
      "Test set: Average loss: 0.031639056533094845\n",
      "Test set: Average loss: 0.03254757640843309\n",
      "Test set: Average loss: 0.03393850951169952\n",
      "Test set: Average loss: 0.038296247546895756\n",
      "Test set: Average loss: 0.03480049573272481\n",
      "Test set: Average loss: 0.0419786729326006\n",
      "Test set: Average loss: 0.031725298841479344\n",
      "Test set: Average loss: 0.03763015881213505\n",
      "Test set: Average loss: 0.03517386395515132\n",
      "Test set: Average loss: 0.029711873491578444\n",
      "Test set: Average loss: 0.03264894298536092\n",
      "Test set: Average loss: 0.037766425501104095\n",
      "Test set: Average loss: 0.0329067225820836\n",
      "Test set: Average loss: 0.03237090773609452\n",
      "Test set: Average loss: 0.039015026721244794\n",
      "Test set: Average loss: 0.032563802106778894\n",
      "Test set: Average loss: 0.03307681400292495\n",
      "Test set: Average loss: 0.03356037322373595\n",
      "Test set: Average loss: 0.03560526668388775\n",
      "Test set: Average loss: 0.041887256774316484\n",
      "Test set: Average loss: 0.03397501534163894\n",
      "Test set: Average loss: 0.03541094594027072\n",
      "Test set: Average loss: 0.030107805071565963\n",
      "Test set: Average loss: 0.035233922694033026\n",
      "Test set: Average loss: 0.031407411374857475\n",
      "Test set: Average loss: 0.03312324994021765\n",
      "Test set: Average loss: 0.02985532811567282\n",
      "Test set: Average loss: 0.03355290701983904\n",
      "Test set: Average loss: 0.033856371989656964\n",
      "Test set: Average loss: 0.03397777283407777\n",
      "Test set: Average loss: 0.03823183373401662\n",
      "Test set: Average loss: 0.033234872924367664\n",
      "Test set: Average loss: 0.03494057416446594\n",
      "Test set: Average loss: 0.028277373300625186\n",
      "Test set: Average loss: 0.03563677594320325\n",
      "Test set: Average loss: 0.03144525374955265\n",
      "Test set: Average loss: 0.03616871356189222\n",
      "Test set: Average loss: 0.031207211188302608\n",
      "Test set: Average loss: 0.030277929585281527\n",
      "Test set: Average loss: 0.02688034843049536\n",
      "Test set: Average loss: 0.03142268355259148\n",
      "Test set: Average loss: 0.03442053893377306\n",
      "Test set: Average loss: 0.028478646499024762\n",
      "Test set: Average loss: 0.03208598358287418\n",
      "Test set: Average loss: 0.037994195825376664\n",
      "Test set: Average loss: 0.028675600236965693\n",
      "Test set: Average loss: 0.03139592384400203\n",
      "Test set: Average loss: 0.03681319995394006\n",
      "Test set: Average loss: 0.03370148830811104\n",
      "Test set: Average loss: 0.030877640814651386\n",
      "Test set: Average loss: 0.03461913791761617\n",
      "Test set: Average loss: 0.0328421309186524\n",
      "Test set: Average loss: 0.036782904145638895\n",
      "Test set: Average loss: 0.03475099372826662\n",
      "Test set: Average loss: 0.03993292842642404\n",
      "Test set: Average loss: 0.03719382125905213\n",
      "Test set: Average loss: 0.030974934372352436\n",
      "Test set: Average loss: 0.03663890446841833\n",
      "Test set: Average loss: 0.03583902278660389\n",
      "Test set: Average loss: 0.02904773535192362\n",
      "Test set: Average loss: 0.03213501399841334\n",
      "Test set: Average loss: 0.033227078994968906\n",
      "Test set: Average loss: 0.031074893190520925\n",
      "Test set: Average loss: 0.03140373107180494\n",
      "Test set: Average loss: 0.029736492816937243\n",
      "Test set: Average loss: 0.03667218695500196\n",
      "Test set: Average loss: 0.03553362999832643\n",
      "Test set: Average loss: 0.033458424664786435\n",
      "Test set: Average loss: 0.03029253744171001\n",
      "Test set: Average loss: 0.04135669779134332\n",
      "Test set: Average loss: 0.030391962814064755\n",
      "Test set: Average loss: 0.032714354098970944\n",
      "Test set: Average loss: 0.02979200897971168\n",
      "Test set: Average loss: 0.03447392899533952\n",
      "Test set: Average loss: 0.031392678375596006\n",
      "Test set: Average loss: 0.036332279834823566\n",
      "Test set: Average loss: 0.03078684899264772\n",
      "Test set: Average loss: 0.0293589339889877\n",
      "Test set: Average loss: 0.029654283390300407\n",
      "Test set: Average loss: 0.03019888940389137\n",
      "Test set: Average loss: 0.037384817498605116\n",
      "Test set: Average loss: 0.03365060565101885\n",
      "Test set: Average loss: 0.03299350431734638\n",
      "Test set: Average loss: 0.02951100920108729\n",
      "Test set: Average loss: 0.032563190598693836\n",
      "Test set: Average loss: 0.030972081444815558\n",
      "Test set: Average loss: 0.02878976160172897\n",
      "Test set: Average loss: 0.031274221850253525\n",
      "Test set: Average loss: 0.034405897959004506\n",
      "Test set: Average loss: 0.03471078119400772\n",
      "Test set: Average loss: 0.03399620185882668\n",
      "Test set: Average loss: 0.029506347110327624\n",
      "Test set: Average loss: 0.029498142748707323\n",
      "Test set: Average loss: 0.03384740013416376\n",
      "Test set: Average loss: 0.031828593255568194\n",
      "Test set: Average loss: 0.045906645936838686\n",
      "Test set: Average loss: 0.0318354731243744\n",
      "Test set: Average loss: 0.034907959856354864\n",
      "Test set: Average loss: 0.03368091455376998\n",
      "Test set: Average loss: 0.03179381485142585\n",
      "Test set: Average loss: 0.03175214005932503\n",
      "Test set: Average loss: 0.032300852979460615\n",
      "Test set: Average loss: 0.039133742221565626\n",
      "Test set: Average loss: 0.029434158950607525\n",
      "Test set: Average loss: 0.031097411378596007\n",
      "Test set: Average loss: 0.02976679877610877\n",
      "Test set: Average loss: 0.032970946482237196\n",
      "Test set: Average loss: 0.03243283037591027\n",
      "Test set: Average loss: 0.027341198384874588\n",
      "Test set: Average loss: 0.03646489012794518\n",
      "Test set: Average loss: 0.033723119339356344\n",
      "Test set: Average loss: 0.042866452098678565\n",
      "Test set: Average loss: 0.032343708919142955\n",
      "Test set: Average loss: 0.027762438940044376\n",
      "Test set: Average loss: 0.03056104723327735\n",
      "Test set: Average loss: 0.03650663851021818\n",
      "Test set: Average loss: 0.032796408367090635\n",
      "Test set: Average loss: 0.036355929518094854\n",
      "Test set: Average loss: 0.03127331353061891\n",
      "Test set: Average loss: 0.029051833791527315\n",
      "Test set: Average loss: 0.03238367323683633\n",
      "Test set: Average loss: 0.04007719768333118\n",
      "Test set: Average loss: 0.04083618886807017\n",
      "Test set: Average loss: 0.024709376260943827\n",
      "Test set: Average loss: 0.03229598608049855\n",
      "Test set: Average loss: 0.035759381509706145\n",
      "Test set: Average loss: 0.03255224952044955\n",
      "Test set: Average loss: 0.034910101085188217\n",
      "Test set: Average loss: 0.02772355036722729\n",
      "Test set: Average loss: 0.03654074847327138\n",
      "Test set: Average loss: 0.0363040319416541\n",
      "Test set: Average loss: 0.041716297982475226\n",
      "Test set: Average loss: 0.03151002985123341\n",
      "Test set: Average loss: 0.03186756279501424\n",
      "Test set: Average loss: 0.033474899328211905\n",
      "Test set: Average loss: 0.028582970440766076\n",
      "Test set: Average loss: 0.03140134365639824\n",
      "Test set: Average loss: 0.031448486843146384\n",
      "Test set: Average loss: 0.03196065436168283\n",
      "Test set: Average loss: 0.03224732557828247\n",
      "Test set: Average loss: 0.03027107040361443\n",
      "Test set: Average loss: 0.03725321890851774\n",
      "Test set: Average loss: 0.035993466937725316\n",
      "Test set: Average loss: 0.034380475370653585\n",
      "Test set: Average loss: 0.03233339277619507\n",
      "Test set: Average loss: 0.031336511974586756\n",
      "Test set: Average loss: 0.0348843749147818\n",
      "Test set: Average loss: 0.029681968522709212\n",
      "Test set: Average loss: 0.02832797264673559\n",
      "Test set: Average loss: 0.031813412845622224\n",
      "Test set: Average loss: 0.03813629297474108\n",
      "Test set: Average loss: 0.02881511236228107\n",
      "Test set: Average loss: 0.03502218664289103\n",
      "Test set: Average loss: 0.035038558159612876\n",
      "Test set: Average loss: 0.03093327098122245\n",
      "Test set: Average loss: 0.03386610012148594\n",
      "Test set: Average loss: 0.035935026264951375\n",
      "Test set: Average loss: 0.02907798775640913\n",
      "Test set: Average loss: 0.031233007797709433\n",
      "Test set: Average loss: 0.035552902120628005\n",
      "Test set: Average loss: 0.02994094968380523\n",
      "Test set: Average loss: 0.03177999324816483\n",
      "Test set: Average loss: 0.029680751324121957\n",
      "Test set: Average loss: 0.0317225870621769\n",
      "Test set: Average loss: 0.03847828708370571\n",
      "Test set: Average loss: 0.035872894824024115\n",
      "Test set: Average loss: 0.03230469700474714\n",
      "Test set: Average loss: 0.02749399209733383\n",
      "Test set: Average loss: 0.03361361628958548\n",
      "Test set: Average loss: 0.03003745303794858\n",
      "Test set: Average loss: 0.03521021098458732\n",
      "Test set: Average loss: 0.04432293152058264\n",
      "Test set: Average loss: 0.033804582420998486\n",
      "Test set: Average loss: 0.037575654836473404\n",
      "Test set: Average loss: 0.03359845981685794\n",
      "Test set: Average loss: 0.03766074105533335\n",
      "Test set: Average loss: 0.038785795285321\n",
      "Test set: Average loss: 0.03606594800180574\n",
      "Test set: Average loss: 0.03318972107354057\n",
      "Test set: Average loss: 0.030581065688238596\n",
      "Test set: Average loss: 0.030776132649407373\n",
      "Test set: Average loss: 0.03026853357187065\n",
      "Test set: Average loss: 0.03509603123711713\n",
      "Test set: Average loss: 0.03265761941293022\n",
      "Test set: Average loss: 0.0326161624134329\n",
      "Test set: Average loss: 0.03266015541885281\n",
      "Test set: Average loss: 0.03025788981176447\n",
      "Test set: Average loss: 0.026524648174017784\n",
      "Test set: Average loss: 0.03783171239956573\n",
      "Test set: Average loss: 0.02991513552296965\n",
      "Test set: Average loss: 0.02931641442091859\n",
      "Test set: Average loss: 0.03248330176120362\n",
      "Test set: Average loss: 0.03348988232210104\n",
      "Test set: Average loss: 0.03396275358591083\n",
      "Test set: Average loss: 0.03631631096095589\n",
      "Test set: Average loss: 0.03853839988641994\n",
      "Test set: Average loss: 0.038786163820986985\n",
      "Test set: Average loss: 0.03513787387055345\n",
      "Test set: Average loss: 0.03779302484826985\n",
      "Test set: Average loss: 0.03429729408981075\n",
      "Test set: Average loss: 0.035574126498431724\n",
      "Test set: Average loss: 0.032831945382895356\n",
      "Test set: Average loss: 0.03196749707285562\n",
      "Test set: Average loss: 0.03512989587034099\n",
      "Test set: Average loss: 0.030341466148456675\n",
      "Test set: Average loss: 0.03194714782239316\n",
      "Test set: Average loss: 0.03218824134592069\n",
      "Test set: Average loss: 0.04094443461235642\n",
      "Test set: Average loss: 0.027808360041490232\n",
      "Test set: Average loss: 0.032211908412136836\n",
      "Test set: Average loss: 0.02955832122825086\n",
      "Test set: Average loss: 0.0426089597376631\n",
      "Test set: Average loss: 0.03126227140182891\n",
      "Test set: Average loss: 0.028715641833741756\n",
      "Test set: Average loss: 0.031219883894664235\n",
      "Test set: Average loss: 0.03220164200865838\n",
      "Test set: Average loss: 0.031309003720252804\n",
      "Test set: Average loss: 0.0347828164085513\n",
      "Test set: Average loss: 0.03512164788571681\n",
      "Test set: Average loss: 0.03568996100966615\n",
      "Test set: Average loss: 0.03627018150109507\n",
      "Test set: Average loss: 0.034077409555720806\n",
      "Test set: Average loss: 0.031022200944335054\n",
      "Test set: Average loss: 0.030185278595126874\n",
      "Test set: Average loss: 0.03382747091836791\n",
      "Test set: Average loss: 0.038234116795933915\n",
      "Test set: Average loss: 0.03285432982193015\n",
      "Test set: Average loss: 0.038519914224593776\n",
      "Test set: Average loss: 0.04465672830974654\n",
      "Test set: Average loss: 0.03390047653556394\n",
      "Test set: Average loss: 0.030958112075268218\n",
      "Test set: Average loss: 0.032657096335242386\n",
      "Test set: Average loss: 0.03123900533341839\n",
      "Test set: Average loss: 0.03556075155574945\n",
      "Test set: Average loss: 0.031129071922578078\n",
      "Test set: Average loss: 0.032353899729059776\n",
      "Test set: Average loss: 0.030454724641458597\n",
      "Test set: Average loss: 0.03755702339003619\n",
      "Test set: Average loss: 0.03357793373288587\n",
      "Test set: Average loss: 0.04135230080100882\n",
      "Test set: Average loss: 0.03417929659190122\n",
      "Test set: Average loss: 0.03605201769460109\n",
      "Test set: Average loss: 0.03269567068582546\n",
      "Test set: Average loss: 0.03445084517261421\n",
      "Test set: Average loss: 0.035173481392121175\n",
      "Test set: Average loss: 0.03219433784033754\n",
      "Test set: Average loss: 0.03645066857279744\n",
      "Test set: Average loss: 0.031162961802692735\n",
      "Test set: Average loss: 0.03634606265586626\n",
      "Test set: Average loss: 0.035294417254135624\n",
      "Test set: Average loss: 0.03261629395808541\n",
      "Test set: Average loss: 0.029993747935805004\n",
      "Test set: Average loss: 0.033041253785086155\n",
      "Test set: Average loss: 0.031860176557529485\n",
      "Test set: Average loss: 0.031872495815150614\n",
      "Test set: Average loss: 0.03772049415147194\n",
      "Test set: Average loss: 0.03736063658470812\n",
      "Test set: Average loss: 0.02653096665017074\n",
      "Test set: Average loss: 0.03494854269956704\n",
      "Test set: Average loss: 0.03311310864864936\n",
      "Test set: Average loss: 0.044144230132587836\n",
      "Test set: Average loss: 0.03250089146786195\n",
      "Test set: Average loss: 0.030098285825260973\n",
      "Test set: Average loss: 0.03260595144092804\n",
      "Test set: Average loss: 0.03418448423144582\n",
      "Test set: Average loss: 0.03853436710414826\n",
      "Test set: Average loss: 0.03513449046840833\n",
      "Test set: Average loss: 0.0341379239048365\n",
      "Test set: Average loss: 0.03304903667230974\n",
      "Test set: Average loss: 0.031786258680881474\n",
      "Test set: Average loss: 0.03698530909514375\n",
      "Test set: Average loss: 0.038240125077209086\n",
      "Test set: Average loss: 0.03489467624149256\n",
      "Test set: Average loss: 0.033383277977918624\n",
      "Test set: Average loss: 0.03532076522697025\n",
      "Test set: Average loss: 0.034085587988556654\n",
      "Test set: Average loss: 0.034068532514538674\n",
      "Test set: Average loss: 0.03465958889319154\n",
      "Test set: Average loss: 0.03341757479120133\n",
      "Test set: Average loss: 0.028402743721926527\n",
      "Test set: Average loss: 0.03574988820673752\n",
      "Test set: Average loss: 0.037694009792176075\n",
      "Test set: Average loss: 0.034425332203682046\n",
      "Test set: Average loss: 0.03631896300430526\n",
      "Test set: Average loss: 0.03130046612022852\n",
      "Test set: Average loss: 0.032317785578925395\n",
      "Test set: Average loss: 0.05185481150374471\n",
      "Test set: Average loss: 0.04166258442819526\n",
      "Test set: Average loss: 0.03371084675518432\n",
      "Test set: Average loss: 0.037130874565264094\n",
      "Test set: Average loss: 0.030917144775230554\n",
      "Test set: Average loss: 0.03514555574838596\n",
      "Test set: Average loss: 0.03434230794073301\n",
      "Test set: Average loss: 0.042116812259337166\n",
      "Test set: Average loss: 0.035189830352464924\n",
      "Test set: Average loss: 0.02748549563693814\n",
      "Test set: Average loss: 0.03652988035173621\n",
      "Test set: Average loss: 0.03247417762963778\n",
      "Test set: Average loss: 0.028107013206863485\n",
      "Test set: Average loss: 0.03432838942626404\n",
      "Test set: Average loss: 0.03263453667932481\n",
      "Test set: Average loss: 0.029343761638301658\n",
      "Test set: Average loss: 0.0345562220127249\n",
      "Test set: Average loss: 0.03545542070060037\n",
      "Test set: Average loss: 0.027565371805394534\n",
      "Test set: Average loss: 0.029825816742231837\n",
      "Test set: Average loss: 0.02906632649637686\n",
      "Test set: Average loss: 0.02802365640673088\n",
      "Test set: Average loss: 0.028808024903810292\n",
      "Test set: Average loss: 0.035466916649511404\n",
      "Test set: Average loss: 0.03552876234789437\n",
      "Test set: Average loss: 0.03319860464398516\n",
      "Test set: Average loss: 0.027837015886689187\n",
      "Test set: Average loss: 0.032934147806372494\n",
      "Test set: Average loss: 0.03402925716000027\n",
      "Test set: Average loss: 0.03404761816204882\n",
      "Test set: Average loss: 0.02852542865821306\n",
      "Test set: Average loss: 0.03730685454866034\n",
      "Test set: Average loss: 0.030544044448106433\n",
      "Test set: Average loss: 0.030061265237236512\n",
      "Test set: Average loss: 0.04695100112076034\n",
      "Test set: Average loss: 0.03140673045982112\n",
      "Test set: Average loss: 0.03666869469725498\n",
      "Test set: Average loss: 0.0357605804783816\n",
      "Test set: Average loss: 0.0351451976748649\n",
      "Test set: Average loss: 0.03171103643308015\n",
      "Test set: Average loss: 0.04274860404257197\n",
      "Test set: Average loss: 0.03178985854174243\n",
      "Test set: Average loss: 0.03339499899993825\n",
      "Test set: Average loss: 0.035681823874256224\n",
      "Test set: Average loss: 0.03285678680094861\n",
      "Test set: Average loss: 0.03628708782935064\n",
      "Test set: Average loss: 0.03427905766693584\n",
      "Test set: Average loss: 0.0316419654946003\n",
      "Test set: Average loss: 0.039001896856916574\n",
      "Test set: Average loss: 0.040287850090862776\n",
      "Test set: Average loss: 0.030141660317895003\n",
      "Test set: Average loss: 0.03079134389099636\n",
      "Test set: Average loss: 0.03256473542342064\n",
      "Test set: Average loss: 0.036382893225891166\n",
      "Test set: Average loss: 0.03675930673625771\n",
      "Test set: Average loss: 0.02823756808129474\n",
      "Test set: Average loss: 0.03891895282822588\n",
      "Test set: Average loss: 0.03293889474389289\n",
      "Test set: Average loss: 0.031890674080386816\n",
      "Test set: Average loss: 0.028144580852313084\n",
      "Test set: Average loss: 0.029027941318418016\n",
      "Test set: Average loss: 0.02874445995826136\n",
      "Test set: Average loss: 0.03167999507331842\n",
      "Test set: Average loss: 0.03139810011634836\n",
      "Test set: Average loss: 0.03416985836474851\n",
      "Test set: Average loss: 0.03378526301094098\n",
      "Test set: Average loss: 0.033629462157477974\n",
      "Test set: Average loss: 0.031224949772877153\n",
      "Test set: Average loss: 0.03717302380073306\n",
      "Test set: Average loss: 0.04674419119692175\n",
      "Test set: Average loss: 0.031350497999483196\n",
      "Test set: Average loss: 0.03552799588396738\n",
      "Test set: Average loss: 0.0315380735846702\n",
      "Test set: Average loss: 0.038607207032328006\n",
      "Test set: Average loss: 0.03441186786039907\n",
      "Test set: Average loss: 0.034157933519054495\n",
      "Test set: Average loss: 0.03656671547196311\n",
      "Test set: Average loss: 0.03312085747506899\n",
      "Test set: Average loss: 0.03225702804593311\n",
      "Test set: Average loss: 0.033491385669549345\n",
      "Test set: Average loss: 0.03088635654785321\n",
      "Test set: Average loss: 0.03248657943913713\n",
      "Test set: Average loss: 0.0321888490470883\n",
      "Test set: Average loss: 0.029553059354839206\n",
      "Test set: Average loss: 0.033331216602164204\n",
      "Test set: Average loss: 0.03368119209608267\n",
      "Test set: Average loss: 0.029661664413652034\n",
      "Test set: Average loss: 0.026463824885468057\n",
      "Test set: Average loss: 0.02338613941992662\n",
      "Test set: Average loss: 0.03482377397722303\n",
      "Test set: Average loss: 0.0395713814609735\n",
      "Test set: Average loss: 0.03749425448640658\n",
      "Train Epoch: 2 [0/154850 (0%)]\tLoss: 0.036748\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m name \u001b[38;5;241m=\u001b[39m clone_ppo_model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m      7\u001b[0m directory \u001b[38;5;241m=\u001b[39m directory \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m policy_model \u001b[38;5;241m=\u001b[39m \u001b[43mpretrain_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstudent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mclone_ppo_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_gamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_cuda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mppo_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclone_ppo_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirectory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpretraining.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirectory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpretraining_test.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirectory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpretraining_loss.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loss_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirectory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpretraining_test_loss.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     24\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 122\u001b[0m, in \u001b[0;36mpretrain_agent\u001b[1;34m(student, batch_size, epochs, scheduler_gamma, learning_rate, log_interval, no_cuda, seed, test_batch_size, csv_file, test_file, loss_file, test_loss_file, ppo_model)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Now we are finally ready to train the policy model.\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 122\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m     test(model, device, test_loader)\n\u001b[0;32m    125\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[16], line 50\u001b[0m, in \u001b[0;36mpretrain_agent.<locals>.train\u001b[1;34m(model, device, train_loader, optimizer)\u001b[0m\n\u001b[0;32m     46\u001b[0m current_observation, target_action \u001b[38;5;241m=\u001b[39m current_observation\u001b[38;5;241m.\u001b[39mto(device), target_action\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     47\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 50\u001b[0m action, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_observation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m action_prediction \u001b[38;5;241m=\u001b[39m action\u001b[38;5;241m.\u001b[39mdouble()\n\u001b[0;32m     53\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(action_prediction, target_action)\n",
      "File \u001b[1;32mc:\\Users\\spige\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\spige\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\spige\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\policies.py:617\u001b[0m, in \u001b[0;36mActorCriticPolicy.forward\u001b[1;34m(self, obs, deterministic)\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    610\u001b[0m \u001b[38;5;124;03mForward pass in all the networks (actor and critic)\u001b[39;00m\n\u001b[0;32m    611\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;124;03m:return: action, value and log probability of the action\u001b[39;00m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;66;03m# Preprocess the observation if needed\u001b[39;00m\n\u001b[1;32m--> 617\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor:\n\u001b[0;32m    619\u001b[0m     latent_pi, latent_vf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_extractor(features)\n",
      "File \u001b[1;32mc:\\Users\\spige\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\policies.py:640\u001b[0m, in \u001b[0;36mActorCriticPolicy.extract_features\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;124;03mPreprocess the observation if needed and extract features.\u001b[39;00m\n\u001b[0;32m    635\u001b[0m \n\u001b[0;32m    636\u001b[0m \u001b[38;5;124;03m:param obs: Observation\u001b[39;00m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;124;03m:return: the output of the features extractor(s)\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor:\n\u001b[1;32m--> 640\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures_extractor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    642\u001b[0m     pi_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mextract_features(obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpi_features_extractor)\n",
      "File \u001b[1;32mc:\\Users\\spige\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\policies.py:131\u001b[0m, in \u001b[0;36mBaseModel.extract_features\u001b[1;34m(self, obs, features_extractor)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03mPreprocess the observation if needed and extract features.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m :return: The extracted features\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    130\u001b[0m preprocessed_obs \u001b[38;5;241m=\u001b[39m preprocess_obs(obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space, normalize_images\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_images)\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfeatures_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed_obs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\spige\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\spige\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\spige\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\torch_layers.py:106\u001b[0m, in \u001b[0;36mNatureCNN.forward\u001b[1;34m(self, observations)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, observations: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\spige\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\spige\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\spige\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\spige\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\spige\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\spige\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\spige\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "directory = \"../data/RL/\"\n",
    "\n",
    "# get name of \"clone_ppo_model\" instance\n",
    "\n",
    "name = clone_ppo_model.__class__.__name__\n",
    "\n",
    "directory = directory + name + \"_\"\n",
    "\n",
    "policy_model = pretrain_agent(\n",
    "    student = clone_ppo_model,\n",
    "    epochs = EPOCHS,\n",
    "    scheduler_gamma=0.7,\n",
    "    learning_rate=1.0,\n",
    "    log_interval=100,\n",
    "    no_cuda=True,\n",
    "    seed=1,\n",
    "    batch_size=64,\n",
    "    test_batch_size=64,\n",
    "    ppo_model=clone_ppo_model,\n",
    "    csv_file=directory + \"pretraining.csv\",\n",
    "    test_file=directory + \"pretraining_test.csv\",\n",
    "    loss_file=directory + \"pretraining_loss.csv\",\n",
    "    test_loss_file=directory + \"pretraining_test_loss.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.  0.]\n",
      "[1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space.low)\n",
    "print(env.action_space.high)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
