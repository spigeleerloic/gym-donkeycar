{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'donkey_environment'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 27\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# path = r\"c:\\Users\\spige\\memoire\\gym-donkeycar-retry\\gym-donkeycar\\CarConsumptionModel\"\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# sys.path.insert(0, path)\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdonkey_environment\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mConsumptionWrapper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConsumptionWrapper\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CustomProgressBarCallback, SaveObservations\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mExpertDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExpertDataSet\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'donkey_environment'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from stable_baselines3 import PPO, SAC, TD3, DDPG, A2C, DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from gym_donkeycar.envs.donkey_env import DonkeyEnv\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, CallbackList\n",
    "\n",
    "import sys\n",
    "\n",
    "from torch.utils.data.dataset import Dataset, random_split\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import gym\n",
    "\n",
    "# path = r\"c:\\Users\\spige\\memoire\\gym-donkeycar-retry\\gym-donkeycar\\CarConsumptionModel\"\n",
    "# sys.path.insert(0, path)\n",
    "\n",
    "from donkey_environment.ConsumptionWrapper import ConsumptionWrapper\n",
    "from utils.callbacks import CustomProgressBarCallback, SaveObservations\n",
    "from utils.ExpertDataset import ExpertDataSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.8\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#! pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# print(th.version.cuda)\n",
    "# print(th.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creation of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting DonkeyGym env\n",
      "Setting default: start_delay 3.0\n",
      "Setting default: max_cte 8.0\n",
      "Setting default: frame_skip 1\n",
      "Setting default: cam_resolution (120, 160, 3)\n",
      "Setting default: log_level 20\n",
      "Setting default: host localhost\n",
      "Setting default: port 9091\n",
      "Setting default: steer_limit 1.0\n",
      "Setting default: throttle_min 0.0\n",
      "Setting default: throttle_max 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\spige\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\spaces\\box.py:78: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "env = ConsumptionWrapper(level=\"steep-ascent\")\n",
    "  \n",
    "name = \"default_reward\"\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "  save_freq=1000,\n",
    "  save_path=\"../models/\",\n",
    "  name_prefix=f\"{name}\",\n",
    "  save_replay_buffer=True,  \n",
    "  save_vecnormalize=True,\n",
    ")\n",
    "\n",
    "callback = CallbackList([checkpoint_callback, CustomProgressBarCallback()])\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalization methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Action: tensor([0.5000, 0.5000], dtype=torch.float64)\n",
      "Denormalized Action: tensor([0.5000, 0.5000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def normalize_actions(action: np.ndarray) -> np.ndarray:\n",
    "     # [-1, -1], [1,1]\n",
    "    \n",
    "    low, high = np.array([-1, -1]), np.array([1, 1])\n",
    "    predicted_action = 2 * (action - low) / (high - low) - 1\n",
    "    return predicted_action\n",
    "\n",
    "def denormalize_actions(normalized_action: np.ndarray) -> np.ndarray:\n",
    "    low, high = np.array([-1, -1]), np.array([1, 1])\n",
    "    denormalized_action = (normalized_action + 1) / 2 * (high - low) + low\n",
    "    return denormalized_action\n",
    "\n",
    "\n",
    "# Example action\n",
    "action = np.array([0.5, 0.5])\n",
    "# convert to tensor\n",
    "action = th.tensor(action)\n",
    "\n",
    "# Normalize the action\n",
    "normalized_action = normalize_actions(action)\n",
    "print(\"Normalized Action:\", normalized_action)\n",
    "\n",
    "# Denormalize the action\n",
    "denormalized_action = denormalize_actions(normalized_action)\n",
    "print(\"Denormalized Action:\", denormalized_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load observation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " observation shape : (193563, 120, 160, 3) \n",
      "\n",
      " action shape : (193563, 2) \n",
      "\n",
      "Train dataset size: 154850 \n",
      "Test dataset size: 38713\n"
     ]
    }
   ],
   "source": [
    "expert_dataset = ExpertDataSet()\n",
    "expert_dataset.load_dataset(\"../data/rollout/dataset_clamped.npz\")\n",
    "\n",
    "print(f\" observation shape : {expert_dataset.observations.shape} \\n\")\n",
    "print(f\" action shape : {expert_dataset.actions.shape} \\n\")\n",
    "\n",
    "def transform_observation(obs: np.ndarray) -> np.ndarray:\n",
    "    # transform (1332, 120, 160, 3) to (1332, 3, 120, 160)\n",
    "    transformed_obs = np.transpose(obs, (0, 3, 1, 2))\n",
    "    return transformed_obs\n",
    "\n",
    "expert_dataset.observations = transform_observation(expert_dataset.observations)\n",
    "train_size = int(0.8 * len(expert_dataset))\n",
    "\n",
    "test_size = len(expert_dataset) - train_size\n",
    "\n",
    "train_expert_dataset, test_expert_dataset = random_split(\n",
    "    expert_dataset, [train_size, test_size]\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size: {train_size} \\n\"\n",
    "        f\"Test dataset size: {test_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\spige\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clone_ppo_model = PPO(\"CnnPolicy\", env, verbose=1, seed=42)\n",
    "EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_agent(\n",
    "    student,\n",
    "    batch_size=64,\n",
    "    epochs=1000,\n",
    "    scheduler_gamma=0.7,\n",
    "    learning_rate=1.0,\n",
    "    log_interval=100,\n",
    "    no_cuda=True,\n",
    "    seed=1,\n",
    "    test_batch_size=64,\n",
    "    csv_file=\"../data/RL/pretraining.csv\",\n",
    "    test_file=\"../data/RL/pretraining_test.csv\",\n",
    "    loss_file=\"../data/RL/pretraining_loss.csv\",\n",
    "    test_loss_file=\"../data/RL/pretraining_test_loss.csv\",\n",
    "    ppo_model=clone_ppo_model,\n",
    "):\n",
    "    use_cuda = not no_cuda and th.cuda.is_available()\n",
    "    th.manual_seed(seed)\n",
    "    device = th.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    kwargs = {\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {}\n",
    "    print(f\"Using device {device}\") \n",
    "    \n",
    "    criterion = nn.L1Loss()\n",
    "    #criterion = nn.MSELoss()\n",
    "\n",
    "    # Extract initial policy\n",
    "    model = student.policy.to(device)\n",
    "\n",
    "    file = open(csv_file, \"w\")\n",
    "    file.write(\"observed_steering,observed_throttle,target_steering,target_throttle\\n\")\n",
    "\n",
    "    test_file = open(test_file, \"w\")\n",
    "    test_file.write(\"observed_steering,observed_throttle,target_steering,target_throttle\\n\")\n",
    "\n",
    "    file_loss = open(loss_file, \"w\")\n",
    "    file_loss.write(\"loss\\n\")\n",
    "\n",
    "    test_file_loss = open(test_loss_file, \"w\")\n",
    "    test_file_loss.write(\"loss\\n\")\n",
    "\n",
    "    def train(model, device, train_loader, optimizer):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (current_observation, target_action) in enumerate(train_loader):\n",
    "            current_observation, target_action = current_observation.to(device), target_action.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if isinstance(env.action_space, gym.spaces.Box):\n",
    "                # A2C/PPO policy outputs actions, values, log_prob\n",
    "                # SAC/TD3 policy outputs actions only\n",
    "                if isinstance(student, (A2C, PPO)):\n",
    "                    action, _, _ = model(current_observation)\n",
    "                else:\n",
    "                    # SAC/TD3:\n",
    "                    action = model(current_observation)\n",
    "                action_prediction = action.double()\n",
    "            else:\n",
    "                # Retrieve the logits for A2C/PPO when using discrete actions\n",
    "                dist = model.get_distribution(current_observation)\n",
    "                action_prediction = dist.distribution.logits\n",
    "                target_action = target_action.long()\n",
    "\n",
    "            loss = criterion(action_prediction, target_action)\n",
    "            file_loss.write(f\"{loss.item()}\\n\")\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            for observed_action, target in zip(action_prediction, target_action):\n",
    "                # extract numpy scalars from PyTorch tensors\n",
    "                observed_action = observed_action.cpu().detach().numpy()\n",
    "                target = target.cpu().detach().numpy()\n",
    "\n",
    "                observed_action = \",\".join(map(str, observed_action))\n",
    "                target = \",\".join(map(str, target))\n",
    "                \n",
    "                file.write(f\"{observed_action},{target}\\n\")\n",
    "\n",
    "            \n",
    "            if batch_idx % log_interval == 0:\n",
    "                print(\n",
    "                    \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                        epoch,\n",
    "                        batch_idx * len(current_observation),\n",
    "                        len(train_loader.dataset),\n",
    "                        100.0 * batch_idx / len(train_loader),\n",
    "                        loss.item(),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    def test(model, device, test_loader):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with th.no_grad():\n",
    "            for current_observation, target_action in test_loader:\n",
    "                current_observation, target_action = current_observation.to(device), target_action.to(device)\n",
    "\n",
    "                if isinstance(env.action_space, gym.spaces.Box):\n",
    "                    # A2C/PPO policy outputs actions, values, log_prob\n",
    "                    # SAC/TD3 policy outputs actions only\n",
    "                    if isinstance(student, (A2C, PPO)):\n",
    "                        action, _, _ = model(current_observation)\n",
    "                    else:\n",
    "                        # SAC/TD3:\n",
    "                        action = model(current_observation)\n",
    "                    action_prediction = action.double()\n",
    "                else:\n",
    "                    # Retrieve the logits for A2C/PPO when using discrete actions\n",
    "                    dist = model.get_distribution(current_observation)\n",
    "                    action_prediction = dist.distribution.logits\n",
    "                    target_action = target_action.long()\n",
    "                \n",
    "                test_loss = criterion(action_prediction, target_action)\n",
    "                for observed_action, target in zip(action_prediction, target_action):\n",
    "                    # extract numpy scalars from PyTorch tensors\n",
    "                    observed_action = observed_action.cpu().detach().numpy()\n",
    "                    target = target.cpu().detach().numpy()\n",
    "\n",
    "                    observed_action = \",\".join(map(str, observed_action))\n",
    "                    target = \",\".join(map(str, target))\n",
    "                    \n",
    "                    test_file.write(f\"{observed_action},{target}\\n\")\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print(f\"Test set: Average loss: {test_loss}\")\n",
    "        test_file_loss.write(f\"{test_loss}\\n\")\n",
    "\n",
    "    # Here, we use PyTorch `DataLoader` to our load previously created `ExpertDataset` for training\n",
    "    # and testing\n",
    "\n",
    "    train_loader = th.utils.data.DataLoader(\n",
    "        dataset=train_expert_dataset, batch_size=batch_size, shuffle=True, **kwargs\n",
    "    )\n",
    "    test_loader = th.utils.data.DataLoader(\n",
    "        dataset=test_expert_dataset,\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=True,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    # Define an Optimizer and a learning rate schedule.\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=scheduler_gamma)\n",
    "\n",
    "    # Now we are finally ready to train the policy model.\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer)\n",
    "        test(model, device, test_loader)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # save the model policy\n",
    "        th.save(model.state_dict(), f\"../models/pretrained_{epoch}.pt\")\n",
    "        ppo_model.policy.load_state_dict(model.state_dict())    \n",
    "\n",
    "        ppo_model.save(f\"../models/pretrained_ppo_{epoch}\")\n",
    "\n",
    "    file.close()\n",
    "    file_loss.close()\n",
    "    test_file_loss.close()\n",
    "\n",
    "    # Implant the trained policy network back into the RL student agent\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n",
      "Train Epoch: 1 [0/154850 (0%)]\tLoss: 0.875500\n",
      "Train Epoch: 1 [6400/154850 (4%)]\tLoss: 0.642247\n",
      "Train Epoch: 1 [12800/154850 (8%)]\tLoss: 0.415114\n",
      "Train Epoch: 1 [19200/154850 (12%)]\tLoss: 0.286912\n",
      "Train Epoch: 1 [25600/154850 (17%)]\tLoss: 0.206924\n",
      "Train Epoch: 1 [32000/154850 (21%)]\tLoss: 0.149094\n",
      "Train Epoch: 1 [38400/154850 (25%)]\tLoss: 0.113525\n",
      "Train Epoch: 1 [44800/154850 (29%)]\tLoss: 0.080471\n",
      "Train Epoch: 1 [51200/154850 (33%)]\tLoss: 0.064197\n",
      "Train Epoch: 1 [57600/154850 (37%)]\tLoss: 0.063418\n",
      "Train Epoch: 1 [64000/154850 (41%)]\tLoss: 0.054908\n",
      "Train Epoch: 1 [70400/154850 (45%)]\tLoss: 0.054045\n",
      "Train Epoch: 1 [76800/154850 (50%)]\tLoss: 0.043295\n",
      "Train Epoch: 1 [83200/154850 (54%)]\tLoss: 0.036112\n",
      "Train Epoch: 1 [89600/154850 (58%)]\tLoss: 0.045701\n",
      "Train Epoch: 1 [96000/154850 (62%)]\tLoss: 0.039002\n",
      "Train Epoch: 1 [102400/154850 (66%)]\tLoss: 0.036322\n",
      "Train Epoch: 1 [108800/154850 (70%)]\tLoss: 0.034139\n",
      "Train Epoch: 1 [115200/154850 (74%)]\tLoss: 0.046175\n",
      "Train Epoch: 1 [121600/154850 (79%)]\tLoss: 0.048104\n",
      "Train Epoch: 1 [128000/154850 (83%)]\tLoss: 0.035484\n",
      "Train Epoch: 1 [134400/154850 (87%)]\tLoss: 0.038120\n",
      "Train Epoch: 1 [140800/154850 (91%)]\tLoss: 0.037303\n",
      "Train Epoch: 1 [147200/154850 (95%)]\tLoss: 0.033710\n",
      "Train Epoch: 1 [153600/154850 (99%)]\tLoss: 0.041962\n",
      "Test set: Average loss: 9.095861105553623e-07\n",
      "Train Epoch: 2 [0/154850 (0%)]\tLoss: 0.034201\n",
      "Train Epoch: 2 [6400/154850 (4%)]\tLoss: 0.033253\n",
      "Train Epoch: 2 [12800/154850 (8%)]\tLoss: 0.030609\n",
      "Train Epoch: 2 [19200/154850 (12%)]\tLoss: 0.028126\n",
      "Train Epoch: 2 [25600/154850 (17%)]\tLoss: 0.035507\n",
      "Train Epoch: 2 [32000/154850 (21%)]\tLoss: 0.032546\n",
      "Train Epoch: 2 [38400/154850 (25%)]\tLoss: 0.026337\n",
      "Train Epoch: 2 [44800/154850 (29%)]\tLoss: 0.027699\n",
      "Train Epoch: 2 [51200/154850 (33%)]\tLoss: 0.032054\n",
      "Train Epoch: 2 [57600/154850 (37%)]\tLoss: 0.030395\n",
      "Train Epoch: 2 [64000/154850 (41%)]\tLoss: 0.029327\n",
      "Train Epoch: 2 [70400/154850 (45%)]\tLoss: 0.029001\n",
      "Train Epoch: 2 [76800/154850 (50%)]\tLoss: 0.031804\n",
      "Train Epoch: 2 [83200/154850 (54%)]\tLoss: 0.025761\n",
      "Train Epoch: 2 [89600/154850 (58%)]\tLoss: 0.030108\n",
      "Train Epoch: 2 [96000/154850 (62%)]\tLoss: 0.036749\n",
      "Train Epoch: 2 [102400/154850 (66%)]\tLoss: 0.033222\n",
      "Train Epoch: 2 [108800/154850 (70%)]\tLoss: 0.031297\n",
      "Train Epoch: 2 [115200/154850 (74%)]\tLoss: 0.028688\n",
      "Train Epoch: 2 [121600/154850 (79%)]\tLoss: 0.038058\n",
      "Train Epoch: 2 [128000/154850 (83%)]\tLoss: 0.037024\n",
      "Train Epoch: 2 [134400/154850 (87%)]\tLoss: 0.031462\n",
      "Train Epoch: 2 [140800/154850 (91%)]\tLoss: 0.039634\n",
      "Train Epoch: 2 [147200/154850 (95%)]\tLoss: 0.037207\n",
      "Train Epoch: 2 [153600/154850 (99%)]\tLoss: 0.029836\n",
      "Test set: Average loss: 6.798119003736328e-07\n",
      "Train Epoch: 3 [0/154850 (0%)]\tLoss: 0.029248\n",
      "Train Epoch: 3 [6400/154850 (4%)]\tLoss: 0.025822\n",
      "Train Epoch: 3 [12800/154850 (8%)]\tLoss: 0.029998\n",
      "Train Epoch: 3 [19200/154850 (12%)]\tLoss: 0.026191\n",
      "Train Epoch: 3 [25600/154850 (17%)]\tLoss: 0.027277\n",
      "Train Epoch: 3 [32000/154850 (21%)]\tLoss: 0.028268\n",
      "Train Epoch: 3 [38400/154850 (25%)]\tLoss: 0.023189\n",
      "Train Epoch: 3 [44800/154850 (29%)]\tLoss: 0.022842\n",
      "Train Epoch: 3 [51200/154850 (33%)]\tLoss: 0.027422\n",
      "Train Epoch: 3 [57600/154850 (37%)]\tLoss: 0.030004\n",
      "Train Epoch: 3 [64000/154850 (41%)]\tLoss: 0.032567\n",
      "Train Epoch: 3 [70400/154850 (45%)]\tLoss: 0.029272\n",
      "Train Epoch: 3 [76800/154850 (50%)]\tLoss: 0.030383\n",
      "Train Epoch: 3 [83200/154850 (54%)]\tLoss: 0.034806\n",
      "Train Epoch: 3 [89600/154850 (58%)]\tLoss: 0.033280\n",
      "Train Epoch: 3 [96000/154850 (62%)]\tLoss: 0.026992\n",
      "Train Epoch: 3 [102400/154850 (66%)]\tLoss: 0.023816\n",
      "Train Epoch: 3 [108800/154850 (70%)]\tLoss: 0.036629\n",
      "Train Epoch: 3 [115200/154850 (74%)]\tLoss: 0.028978\n",
      "Train Epoch: 3 [121600/154850 (79%)]\tLoss: 0.034243\n",
      "Train Epoch: 3 [128000/154850 (83%)]\tLoss: 0.025164\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/RL/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m policy_model \u001b[38;5;241m=\u001b[39m \u001b[43mpretrain_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstudent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mclone_ppo_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_gamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_cuda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mppo_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclone_ppo_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirectory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpretraining.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirectory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpretraining_test.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirectory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpretraining_loss.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loss_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirectory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpretraining_test_loss.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     17\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 145\u001b[0m, in \u001b[0;36mpretrain_agent\u001b[1;34m(student, batch_size, epochs, scheduler_gamma, learning_rate, log_interval, no_cuda, seed, test_batch_size, csv_file, test_file, loss_file, test_loss_file, ppo_model)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Now we are finally ready to train the policy model.\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 145\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m     test(model, device, test_loader)\n\u001b[0;32m    148\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[6], line 45\u001b[0m, in \u001b[0;36mpretrain_agent.<locals>.train\u001b[1;34m(model, device, train_loader, optimizer)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(model, device, train_loader, optimizer):\n\u001b[0;32m     43\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (current_observation, target_action) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m     46\u001b[0m         current_observation, target_action \u001b[38;5;241m=\u001b[39m current_observation\u001b[38;5;241m.\u001b[39mto(device), target_action\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     47\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\spige\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\spige\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\spige\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\spige\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:277\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\spige\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:144\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\spige\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:144\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\spige\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:121\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 121\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\spige\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:183\u001b[0m, in \u001b[0;36mcollate_numpy_array_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np_str_obj_array_pattern\u001b[38;5;241m.\u001b[39msearch(elem\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mstr) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[1;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\spige\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:121\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 121\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\spige\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:174\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    172\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    173\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "directory = \"../data/RL/\"\n",
    "policy_model = pretrain_agent(\n",
    "    student = clone_ppo_model,\n",
    "    epochs = EPOCHS,\n",
    "    scheduler_gamma=0.7,\n",
    "    learning_rate=1.0,\n",
    "    log_interval=100,\n",
    "    no_cuda=True,\n",
    "    seed=1,\n",
    "    batch_size=64,\n",
    "    test_batch_size=64,\n",
    "    ppo_model=clone_ppo_model,\n",
    "    csv_file=directory + \"pretraining.csv\",\n",
    "    test_file=directory + \"pretraining_test.csv\",\n",
    "    loss_file=directory + \"pretraining_loss.csv\",\n",
    "    test_loss_file=directory + \"pretraining_test_loss.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.  0.]\n",
      "[1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space.low)\n",
    "print(env.action_space.high)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
